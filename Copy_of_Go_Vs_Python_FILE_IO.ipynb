{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Go_Vs_Python_FILE_IO.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiaqWSuNdtBbkRYFqlod5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhileswar-Komati/100D_ML/blob/master/Copy_of_Go_Vs_Python_FILE_IO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgvmbJIkfNuy"
      },
      "outputs": [],
      "source": [
        "from cassandra.cluster import Cluster, ExecutionProfile, EXEC_PROFILE_DEFAULT\n",
        "from cassandra.policies import WhiteListRoundRobinPolicy, DowngradingConsistencyRetryPolicy\n",
        "from cassandra.query import tuple_factory\n",
        "\n",
        "profile = ExecutionProfile(\n",
        "    load_balancing_policy=WhiteListRoundRobinPolicy(['127.0.0.1']),\n",
        "    retry_policy=DowngradingConsistencyRetryPolicy(),\n",
        "    consistency_level=ConsistencyLevel.LOCAL_QUORUM,\n",
        "    serial_consistency_level=ConsistencyLevel.LOCAL_SERIAL,\n",
        "    request_timeout=15,\n",
        "    row_factory=tuple_factory\n",
        ")\n",
        "cluster = Cluster(execution_profiles={EXEC_PROFILE_DEFAULT: profile})\n",
        "session = cluster.connect()\n",
        "\n",
        "print(session.execute(\"SELECT release_version FROM system.local\").one())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_csv(\"from_2012.csv\")\n",
        "# print(df.shape)\n",
        "csv_reader1 = csv.reader(open(\"from_2012.csv\"))\n",
        "csv_reader2 = csv.reader(open(\"from_2012.csv\"))\n",
        "csv_reader3 = csv.reader(open(\"from_2012.csv\"))\n",
        "a = time.time()\n",
        "\n",
        "with open('file1.csv', 'w') as csvfile1:\n",
        "    csv_writer1 = csv.writer(csvfile1)\n",
        "    for row in csv_reader1:\n",
        "        csv_writer1.writerow(row)\n",
        "\n",
        "b = time.time()\n",
        "\n",
        "# with open('file2.csv', 'w') as csvfile2:\n",
        "#     csv_writer2 = csv.writer(csvfile2)\n",
        "\n",
        "#     i = 0\n",
        "#     rows = []\n",
        "#     for row in csv_reader2:\n",
        "#         i +=1\n",
        "#         rows.append(row)\n",
        "#         if i==10000:\n",
        "#             i = 0\n",
        "#             csv_writer2.writerows(rows)\n",
        "#             rows = []\n",
        "\n",
        "# c = time.time()\n",
        "\n",
        "# chunks = c-b\n",
        "# single = b-a\n",
        "# # print(c-b)\n",
        "# print(chunks, single, chunks>single)\n",
        "\n",
        "# a = time.time()\n",
        "\n",
        "# with open('file1.csv', 'w') as csvfile1:\n",
        "#     csv_writer1 = csv.writer(csvfile1)\n",
        "#     csv_writer1.writerows(csv_reader3)\n",
        "\n",
        "# b = time.time()\n",
        "\n",
        "print(b-a)"
      ],
      "metadata": {
        "id": "h5I0Tm0rfRg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "steps = [2, 10, 20]\n",
        "\n",
        "ind = 0\n",
        "for step in steps:\n",
        "    csv_reader = csv.reader(open(\"from_2012.csv\"))\n",
        "    b = time.time()\n",
        "    file_name = 'file{}.csv'.format(ind)\n",
        "    with open(file_name, 'w') as csvfile2:\n",
        "        csv_writer2 = csv.writer(csvfile2)\n",
        "\n",
        "        i = 0\n",
        "        rows = []\n",
        "        for row in csv_reader:\n",
        "            i +=1\n",
        "            rows.append(row)\n",
        "            if i==step:\n",
        "                i = 0\n",
        "                csv_writer2.writerows(rows)\n",
        "                rows = []\n",
        "    ind += 1\n",
        "    c = time.time()\n",
        "\n",
        "    chunks = c-b\n",
        "    print(chunks, file_name)\n"
      ],
      "metadata": {
        "id": "OP2pFx9sZ4Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "package main\n",
        "\n",
        "import (\n",
        "\t\"encoding/csv\"\n",
        "\t\"fmt\"\n",
        "\t\"log\"\n",
        "\t\"os\"\n",
        "\t\"time\"\n",
        ")\n",
        "\n",
        "func readCsvFile(filePath string) [][]string {\n",
        "\tf, err := os.Open(filePath)\n",
        "\tif err != nil {\n",
        "\t\tlog.Fatal(\"Unable to read input file \"+filePath, err)\n",
        "\t}\n",
        "\tdefer f.Close()\n",
        "\n",
        "\tcsvReader := csv.NewReader(f)\n",
        "\trecords, err := csvReader.ReadAll()\n",
        "\tif err != nil {\n",
        "\t\tlog.Fatal(\"Unable to parse file as CSV for \"+filePath, err)\n",
        "\t}\n",
        "\n",
        "\treturn records\n",
        "}\n",
        "\n",
        "func writeCsvFile(records [][]string, filePath string) {\n",
        "\tf, err := os.Create(filePath)\n",
        "\tif err != nil {\n",
        "\t\tlog.Fatalf(\"failed creating file: %s\", err)\n",
        "\t}\n",
        "\tdefer f.Close()\n",
        "\n",
        "\tcsvwriter := csv.NewWriter(f)\n",
        "\tfor _, record := range records {\n",
        "\t\t_ = csvwriter.Write(record)\n",
        "\t}\n",
        "\tcsvwriter.Flush()\n",
        "}\n",
        "\n",
        "func writeAllCsvFile(records [][]string, filePath string) {\n",
        "\tf, err := os.Create(filePath)\n",
        "\tif err != nil {\n",
        "\t\tlog.Fatalf(\"failed creating file: %s\", err)\n",
        "\t}\n",
        "\tdefer f.Close()\n",
        "\n",
        "\tcsvwriter := csv.NewWriter(f)\n",
        "\tcsvwriter.WriteAll(records)\n",
        "}\n",
        "\n",
        "func main() {\n",
        "\trecords := readCsvFile(\"from_2012.csv\")\n",
        "\tstart := time.Now()\n",
        "\twriteCsvFile(records, \"gofile.csv\")\n",
        "\tend := time.Now()\n",
        "\tfmt.Println(end.Sub(start))\n",
        "\tstart1 := time.Now()\n",
        "\twriteAllCsvFile(records, \"goallfile.csv\")\n",
        "\tend1 := time.Now()\n",
        "\tfmt.Println(end1.Sub(start1))\n",
        "}\n"
      ],
      "metadata": {
        "id": "Wqgnc1atZ5SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "package main\n",
        "\n",
        "import (\n",
        "\t\"fmt\"\n",
        "\t\"github.com/go-gota/gota/dataframe\"\n",
        "\t\"gonum.org/v1/gonum/mat\"\n",
        "\t// \"reflect\"\n",
        "\t\"github.com/pa-m/sklearn/preprocessing\"\n",
        "\t\"strings\"\n",
        "\t\"io/ioutil\"\n",
        ")\n",
        "\n",
        "type matrix struct {\n",
        "\tdataframe.DataFrame\n",
        "}\n",
        "\n",
        "func (m matrix) At(i, j int) float64 {\n",
        "\treturn m.Elem(i, j).Float()\n",
        "}\n",
        "\n",
        "func (m matrix) T() mat.Matrix {\n",
        "\treturn mat.Transpose{Matrix: m}\n",
        "}\n",
        "\n",
        "func getNumericalColumns(df dataframe.DataFrame) dataframe.DataFrame{\n",
        "\tlist_of_datatypes := df.Types()\n",
        "\tvar numerical_datatype_indexes []int\n",
        "\tfor indx, datatype := range list_of_datatypes {\n",
        "\t\tif datatype == \"float\" || datatype == \"int\" {\n",
        "\t\t\tnumerical_datatype_indexes = append(numerical_datatype_indexes, indx)\n",
        "\t\t}\n",
        "\t}\n",
        "\treturn df.Select(numerical_datatype_indexes)\n",
        "}\n",
        "\n",
        "func data_normalization_function(data dataframe.DataFrame, norm_type string, composite_columns []string) {\n",
        "\tfmt.Println(\"In Data Normalization\") \n",
        "\tnorm_type = strings.ToLower(norm_type)\n",
        "\tdata_without_time := data.Drop(composite_columns)\n",
        "\tdata_num := getNumericalColumns(data_without_time)\n",
        "\tdata_num_matrix := matrix{data_num}\n",
        "\tvar scaled_num_data *mat.Dense\n",
        "\tif norm_type == \"minmax\" {\n",
        "\t\tscaler := preprocessing.NewMinMaxScaler([]float64{0, 1})\n",
        "\t\tscaled_num_data, _ = scaler.FitTransform(data_num_matrix, nil)\n",
        "\t} else if norm_type == \"standard\" {\n",
        "\t\tscaler := preprocessing.NewStandardScaler()\n",
        "\t\tscaled_num_data, _ = scaler.FitTransform(data_num_matrix, nil)\n",
        "\t} else if norm_type == \"maxabs\" {\n",
        "\t\tscaler := preprocessing.NewMaxAbsScaler()\n",
        "\t\tscaled_num_data, _ = scaler.FitTransform(data_num_matrix, nil)\n",
        "\t} else if norm_type == \"robust\" {\n",
        "\t\tscaler := preprocessing.NewRobustScaler(true, false, nil)\n",
        "\t\tscaled_num_data, _ = scaler.FitTransform(data_num_matrix, nil)\n",
        "\t}\n",
        "\tfmt.Println(scaled_num_data)\n",
        "\tscaled_num_data_frame := dataframe.LoadMatrix(scaled_num_data)\n",
        "\tnum_column_names := data_num.Names()\n",
        "\tscaled_num_data_frame.SetNames(num_column_names...)\n",
        "\tfmt.Println(scaled_num_data_frame)\n",
        "\tdf_without_num_columns := data.Drop(num_column_names)\n",
        "\tdata = scaled_num_data_frame.CBind(df_without_num_columns)\n",
        "\tfmt.Println(data)\n",
        "}\n",
        "\n",
        "func main() {\n",
        "\tcontent, _ := ioutil.ReadFile(\"sample_go.csv\")\n",
        "\tioContent := strings.NewReader(string(content))\n",
        "\n",
        "\tdf := dataframe.ReadCSV(ioContent)\n",
        "\tnorm_type := \"minmax\"\n",
        "\tvar composite_columns []string\n",
        "\n",
        "\tdata_normalization_function(df, norm_type, composite_columns)\n",
        "}"
      ],
      "metadata": {
        "id": "K7pvQjmiAyGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}